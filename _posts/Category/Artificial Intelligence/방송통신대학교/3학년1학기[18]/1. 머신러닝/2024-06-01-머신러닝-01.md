---
layout:     BLACKCODE
title:      "1강. overview of Supervised Learning"
subtitle:   ""
description: ""
date:       2024-06-01 01:00:00
author:     ""
header-img: "assets/owner/hero/home-bg.jpg"
thumbnail: /assets/owner/blog/thumbs/thumb01.png
tags: []
category: [머신러닝]
comments: false
share: false
---

# 학습목차
1. 개념과 범위
    - 기계학습, 패턴인식, 학습의 종류, 다항식 피팅
3. 확률
    - 베이즈 정리, 확률분포

# 01 개념과 범위
## 1-1 기계학습
- 기계학습의 결과물
    - 생각하는 기계
    - 자율주행, 번역기
    - 기타
- 학습내용
    - 패턴인식
    - 위의 응용 이전에 보다 근본적인 문제

## 1-2 패턴인식
- 패턴 : 일정한 형태나 양식 또는 유형
![img](/assets/category/방송통신대학교/머신러닝/1강/01.png)
- 패턴을 기계가 알아내도록 하는 것이 기계학습의 기본적인 목표

## 1-3 학습의 종류
- 지도학습 supervised learning
    + 목표가 명시적으로 주어짐. 입력 하나에 답(target)하나
    + 우리가 배울 대부분이 여기에 속함(분류, 회귀등)
- 비지도학습 unsupervised learning
    + 목표가 명시적이지 않음
    + 군집화(clustering), 밀도추정등
- 강화학습
    + 보상을 최대화 하기 위한 행동을 찾음

## 1-4 다항식 곡선 피팅
- 모델선택 : 어떤 모델을 쓸 것인가
    + 다항식 모델이 타당한가
    + 다항식 모델을 사용한다면 M을 어떻게 선택할 것인가

`오차함수`를 정의해야 비교가 가능하다.

실제 데이터는 노이즈를 포함한다.
> 모델, 오차함수의 선택은 더 어려워진다.

# 02 확률
## 2-1 베이즈 정리
1. 곱의법칙 p(x,y) = p(y|x)p(x)
    - 동시에 사건이 발생할 확률
    - 사건 x가 발생하고 x사건이 발생했다고 가정했을 시 y가 발생할 확률
2. 합의법칙 p(x)
    - x사건의 발생확률을 구하고자 할때
    - A사건에 의존적임
    - B사건에 의존적임
    - C사건에 의존적임
    - x사건 = A,B,C 사건 모두의 합
    - p(x) = p(x|A) + p(x|B) + p(x|C)
3. p(x,y) = p(y,x) : 두개가 동시에 발생할 확률이므로 순서 상관x
    - = p(y|x)p(x) = p(x|y)p(y)
    - 베이즈정리 : p(y|x) = p(x|y)p(y)/p(x )
    - 
압선 문제에서, 적합한 매개변수 w를 정할 때, 데이터를 바탕으로 w의 분포를 추정할 수 있다.
D를 관측한 후 w에 대한 불확실성을 표현

- p(w|D) = p(D|w)p(w)/p(D)
- 최대가능도(maximum liklihood) : p(D|w)의 최대화

## 2-2 확률 분포
노이즈가 가우시안 분포를 따른다고 할때, 어떻게 해야 가장 좋은 w를 찾을 수 있겠는가
- 가우시안분포의 평균과 분산 확인
    - 가우시안 적분을 이용해서 가우시안 분포의 정규화 확인
- 가능도 함수의 최대화
- 가우시안 분포의 정규화 확인
- 